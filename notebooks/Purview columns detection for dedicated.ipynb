{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Notebook for PURVIEW schema analysis\n",
        "## Requirements\n",
        "1. A **Purview** environment with a dedicated pool database\n",
        "2. A **Synapse** workspace to run that spark notebook (smallest cluster is enough)\n",
        "3. A **service principal** with permission in purview\n",
        "\n",
        "## Configuration\n",
        "1. Populate all the **paramenters** in the first cell\n",
        "2. **Run** all the notebook cells\n",
        "\n",
        "## Results\n",
        "A list of tables with the ammount of columns to verify the absence of schema in some tables. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Parameters \n",
        "workspacename = \"myworkspace\"\n",
        "databasename = \"mydb\"\n",
        "purviewname = \"mypurview\"\n",
        "storageaccount = \"mystorageaccount\"\n",
        "container = \"mycontainer\"\n",
        "keyvaultname = \"mykeyvault\"\n",
        "secretname = \"mysecret\"\n",
        "\n",
        "csvroot = 'abfss://%s@%s.dfs.core.windows.net/%s/' % (storageaccount, container,\"schema_analysis\")\n",
        "\n",
        "#This variable has to be one of both engines -serverless- or -dedicated-\n",
        "enginetype = \"serverless\"\n",
        "\n",
        "tenant_id = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n",
        "client_id = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n",
        "#We should store secrets in a keyvault where only the Workspace MI has read permissions \n",
        "client_secret = TokenLibrary.getSecret(keyvaultname, secretname)\n",
        "debug = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import requests \n",
        "import json\n",
        "import pandas as pd \n",
        "from notebookutils import mssparkutils \n",
        "from pyspark.sql import * \n",
        "from pyspark.sql.functions import * \n",
        "from pyspark.sql.types import *\n",
        "import jmespath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def azuread_auth(tenant_id: str, client_id: str, client_secret: str, resource_url: str):\n",
        "    url = f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n",
        "    payload= f'grant_type=client_credentials&client_id={client_id}&client_secret={client_secret}&resource={resource_url}'\n",
        "    headers = {\n",
        "        'Content-Type': 'application/x-www-form-urlencoded'\n",
        "        }\n",
        "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "    access_token = json.loads(response.text)['access_token']\n",
        "    return access_token\n",
        "\t\n",
        "\t\n",
        "def get_all_assets(path: str, data_catalog_name: str, azuread_access_token: str, max_depth=1):\n",
        "    url = f\"https://{purviewname}.catalog.purview.azure.com/api/browse?api-version=2021-05-01-preview\"\n",
        "    headers = {\n",
        "            'Authorization': f'Bearer {token}',\n",
        "            'Content-Type': 'application/json'\n",
        "            }\n",
        "    payload=\"\"\"{\n",
        "                \"limit\": 100,\n",
        "                \"offset\": null,\n",
        "                \"path\": \"%s\"\n",
        "                }\"\"\" % (path)         \n",
        "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "    #print(response.text)\n",
        "    li = json.loads(response.text)\n",
        "    # Return all files\n",
        "    for x in jmespath.search(\"value\", li):\n",
        "        if jmespath.search(\"isLeaf\", x):\n",
        "            yield x\n",
        "    if max_depth > 1:\n",
        "        for x in jmespath.search(\"value\", li):\n",
        "            if jmespath.search(\"isLeaf\", x):\n",
        "                continue\n",
        "            for y in get_all_assets(jmespath.search(\"path\", x), data_catalog_name, azuread_access_token, max_depth - 1):\n",
        "                yield y\n",
        "    else:\n",
        "        for x in jmespath.search(\"value\", li):\n",
        "            if jmespath.search(\"!isLeaf\", x):\n",
        "                yield x\n",
        "\n",
        "def getcolumns(typeName: str, assetfqn: str, azuread_access_token: str): \n",
        "    prefix = f\"https://{purviewname}.catalog.purview.azure.com/api/atlas/v2/entity/uniqueAttribute/type\"\n",
        "    url = f\"{prefix}/{typeName}?attr:qualifiedName={assetfqn}\" \n",
        "    headers = { \n",
        "            'Authorization': f'Bearer {azuread_access_token}', \n",
        "            'Content-Type': 'application/json' \n",
        "            } \n",
        "    response = requests.request(\"GET\", url, headers=headers) \n",
        "    if debug: \n",
        "        print(response.text)\n",
        "    result = json.loads(response.text)['referredEntities'].values() \n",
        "    #dbschema = json.loads(response.text)['entity']['relationshipAttributes']['dbSchema']['displayText'].values()[0]\n",
        "    mycolumns = [] \n",
        "    for column in result: \n",
        "        item = {} \n",
        "        item['typename'] = column['typeName']\n",
        "        #item['dbschema'] = dbschema\n",
        "        item['name'] = column['attributes']['name'] \n",
        "        item['guid'] = column['guid'] \n",
        "        mycolumns.append(item) \n",
        "    mycolumns = pd.DataFrame.from_dict(mycolumns) \n",
        "    return mycolumns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "token = azuread_auth (tenant_id, client_id, client_secret, \"https://purview.azure.net\")\n",
        "# token = azuread_auth(tenantid,clientid,secret,\"https://purview.azure.net\")\n",
        "\n",
        "# if schema predefined path = f\"/azure_synapse_workspace#syn-mfceus2aaihubtst11/azure_synapse_dedicated_sql_db#{databasename}/azure_synapse_dedicated_sql_schema#{schemaname}\"\n",
        "purviewpath = f\"/azure_synapse_workspace#{workspacename}/azure_synapse_{enginetype}_sql_db#{databasename}\"\n",
        "\n",
        "items = get_all_assets(purviewpath,purviewname,token,max_depth=2)\n",
        "mydict = []\n",
        "for item in items:\n",
        "    mydict.append(item)\n",
        "if len(mydict)>0:\n",
        "    tables=spark.createDataFrame(mydict)\n",
        "else:\n",
        "    print(\"THE TABLE DATA is empty, please check that this database has tables in Purview\")\n",
        "if debug:\n",
        "    display(tables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# baseurl=\"https://\"+mssparkutils.env.getWorkspaceName()+\".dev.azuresynapse.net\"\n",
        "# headers = {\"Authorization\": \"Bearer \"+mssparkutils.credentials.getToken(\"Synapse\")}\n",
        "# constructed_url = baseurl + \"/databases/\"+lakedatabase+\"/tables?api-version=2021-04-01\"\n",
        "# response = requests.get(constructed_url, headers=headers)\n",
        "# response = json.loads(response.text)\n",
        "# print(response)\n",
        "items = 0\n",
        "# tables = spark.sql('show tables in ' + lakedatabase)\n",
        "print(\"          Table       - Count  --  fqn on mssql server / purview \")\n",
        "output = []\n",
        "for item in tables.collect():\n",
        "    outitem = {}\n",
        "    notfinish = True\n",
        "    tablename=item['name']\n",
        "    fqn = item['qualifiedName']\n",
        "    typename = item['entityType']\n",
        "    assetfqn = item['qualifiedName']\n",
        "    #f\"mssql://datamasked-ondemand.sql.azuresynapse.net/{lakedatabase}/dbo/{tablename}\"\n",
        "    token = azuread_auth(tenant_id,client_id,client_secret,\"https://purview.azure.net\")\n",
        "    # result = set_desc(typename, assetfqn,description,token)\n",
        "    try:\n",
        "        dfcolumns = getcolumns(typename, assetfqn,token)\n",
        "        columncount = dfcolumns.shape[0]\n",
        "        if columncount == 0:\n",
        "            outitem['tablename'] = tablename\n",
        "            outitem['columncount'] = columncount\n",
        "            outitem['fqn'] = fqn\n",
        "            output.append(outitem)\n",
        "            items += 1\n",
        "        print(\"%20s\" % (tablename) + f\"  -   {columncount}    -- {fqn}\")\n",
        "    except Exception as e: \n",
        "        print(e)\n",
        "pd.DataFrame(output).to_csv(csvroot + databasename +\"_emptytables.csv\",storage_options = {'linked_service' : 'datamasked-WorkspaceDefaultStorage'})\n",
        "print(f\"\\n Items without columns: {items}\")"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
